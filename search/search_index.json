{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to OFO documentation","text":"<p>Welcome to the documentation for the Open Forest Observatory .</p>"},{"location":"contributing/","title":"Contributing to OFO development","text":"<p>Thanks for your interest in contributing! Our work is all open source and enabled entirely by public funding and community contributions. See our instructions for contributing to code and contributing to documentation.</p>"},{"location":"contributing/code/","title":"Contributing to code","text":"<p>We are currently developing code contributing guidelines. In the meantime, if you are interested in contributing, refer to the contributing guidelines in the repository you are interested in contributing to (example for Geograypher), or contact us.</p>"},{"location":"contributing/docs/","title":"Contributing to documentation","text":"<p>Thanks for your interest in contributing to the OFO documentation. This guide will help you get started.</p> <p>The docs are written in Markdown, with some extensions for tabs and other features. See the MkDocs documentation for details.</p> <p>There are two main options for contributing: via the GitHub web interface or by editing a local copy of the documentation files.</p>"},{"location":"contributing/docs/#github-web-interface","title":"GitHub web interface","text":"<p>This is the easiest way to contribute immediately with no setup (just a GitHub account), but for large changes or numerous successive changes, it can become tedious and end up taking more time.</p> <p>To edit a page, navigate to that page on the docs site and click the \"Edit this page\"  button in the top right corner. This will take you to the GitHub record for that file, in editing mode. (You need to be logged in to GitHub.) Make your changes and then click the \"Commit changes...\" button in the top right. You will get a pop-up box that asks for a commit message describing your change. Enter a message.</p> External contributorsInternal contributors <p>Click \"Propose changes\". This will take you to a page where you can review your changes (\"diffs\") and create a pull request. If you want to make more edits, the easiest way is to go back to the documentation site and click the \"Edit this page\" button again. You will return to the GitHub editing page with your previous changes still there. Once your changes look good, click \"Create pull request\". Optionally customize the pull request title and description to summarize your changes for the docs maintainers, and then click \"Create pull request\" again. You will be taken to the pull request page, where you can see the status of the automated tests and any comments from the maintainers. If the tests pass, a maintainer will review your changes and merge them into the documentation. For detailed background on pull requests, see the GitHub documentation.</p> <p>For simple changes, you can click \"Commit changes\" to commit directly to the main branch. The live docs website should update within a few minutes to reflect your changes.</p> <p>For more complex changes or those you want someone to review, select \"Create a new branch for this commit\" then click \"Propose changes\". This will create a new branch in the docs repo with your changes committed, and will take you to a page where you can review your changes (\"diffs\") and create a pull request. If you want to make more edits before submitting the pull request, you would then need to find that page in the GitHub repo, make sure you're in your new branch, and click the \"Edit this file\" button in the top right. Once your changes look good, click \"Create pull request\" and then tag someone to review it.</p>"},{"location":"contributing/docs/#local-clone-or-fork-of-docs-repository","title":"Local clone or fork of docs repository","text":"<p>If you're going to make more than a simple change or two, it will probably be easier to set up a local clone or fork of the docs repository, so that you can edit the docs files on your local computer, preview them locally, and push your changes to GitHub when you're ready.</p> External contributorsInternal contributors <p>Create a \"fork\" of the docs repository in your own GitHub account by clicking the \"Fork\"  button in the top right of the docs repository home page on GitHub. This will create a copy of the docs repo in your own GitHub account. Then clone your fork to your local computer using <code>git</code> from the command line or the git GUI of your choice.</p> <p>Make your edits locally to the relevant markdown files in the <code>docs</code> folder, using the text editor or IDE of your choice.</p> <p>When you're happy with your changes, commit them to your local clone and push them to your fork on GitHub. Then go to your fork of the OFO Docs repository on GitHub and click the \"New pull request\" button in the top right. Click \"Create pull request\". Optionally customize the pull request title and description to summarize your changes for the docs maintainers, and then click \"Create pull request\" again. You will be taken to the pull request page, where you can see the status of the automated tests and any comments from the maintainers. If the tests pass, a maintainer will review your changes and merge them into the documentation. For detailed background on pull requests, see the GitHub documentation.</p> <p>Clone the docs repo locally</p> <p>For simple changes, you can commit directly to the main branch and push directly to main on GitHub. The live docs website should update within a few minutes to reflect your changes.</p> <p>For more complex changes or those you want someone to review, in your local clone of the docs repo, create a new branch for your changes.</p> <p>Make your edits locally to the relevant markdown files in the <code>docs</code> folder.</p> <p>When you're happy with your changes, commit them to your local branch and push them to your branch of the OFO Docs repository on GitHub. Then create a pull request and either merge it yourself or request a review.</p>"},{"location":"contributing/docs/#previewing-the-docs-site-locally-as-you-edit","title":"Previewing the docs site locally as you edit","text":"<p>Installation: <code>pip install mkdocs mkdocs-material mkdocs-awesome-pages-plugin mkdocs-nav-weight mkdocs-git-revision-date-localized-plugin mkdocs-git-committers-plugin-2</code></p> <p>From your local docs repository directory, run <code>mkdocs serve</code>. This will start a local web server that you can access at http://127.0.0.1:8000/ in your browser. It updates every time you save your changes to a docs file locally.</p>"},{"location":"geograypher/","title":"Geograypher","text":"<p>Geograypher core functionality is documented in its GitHub readme, python docstrings, and via the example notebooks.</p> <p>The geograypher documentation on this site covers:</p> <ul> <li>Using Geograypher in prediction workflows</li> </ul>"},{"location":"geograypher/prediction/","title":"Prediction tasks with Geograypher","text":"<p>Geograypher is used for making predictions about a landscape using multi-view drone data along with field reference surveys.</p>"},{"location":"geograypher/prediction/#workflow","title":"Workflow","text":"<p>The common workflow I've used is as follows.</p>"},{"location":"geograypher/prediction/#photogrammetry","title":"Photogrammetry","text":"<p>Geograypher requires that the drone survey data is pre-processed using photogrammetry. This processing can be done graphically using <code>Agisoft Metashape</code> or can be scripted with automate-metashape. For tasks using the raw images, the camera poses and mesh must be provided. A DTM is optional, but helpful in many applications for identifying the ground points. For the workflows using the orthomosaic, it is the only product that is required.</p>"},{"location":"geograypher/prediction/#standardizing-field-reference-data","title":"Standardizing field reference data","text":"<p>Geograypher expects that the field reference data will be standardized before it can be used. Currently, this means that the data should be in a geospatial format that can be read by <code>geopandas</code>, has a <code>Polygon/MultiPolygon</code> geometry, and a has single column that contains the label you want to predict. For example, in a species classification problem, the user could provide a file with one <code>MultiPolygon</code> per species, with a column called <code>species</code> identifying what species each region corresponds to. Alternatively, they could provide <code>Polygon</code> data per tree, with one entry per tree, and a column called <code>species</code> identifying what species each tree is.</p> <p>Standardizing the field reference data can take many forms, but it problem-specific and can usually be accomplished with standard geospatial tools, either graphical ones such as <code>QGIS</code> or scripts relying on libraries such <code>geopandas</code>. Precise geo-referencing relative to the photogrammetry products is critical, and this may required manual alignment using a graphical tool or scripted corrections based on RTK measurements. The format of the data may not exactly match the required specification. For example, <code>Point</code> data with an associated <code>radius</code> should be transformed into a <code>Polygon</code>-typed circle. Often the label you want to classify is not directly present in the raw field reference data. For example, if you want to both classify species and whether it is alive or dead, you may have to concatenate the information from the two columns to provide a unique label for each situation. Alternatively, you may want to merge labels before prediction, drop a subset of them, or override some labels using another column. All of these operations are easy to do using <code>geopandas</code>, and presumably possible in other tools.</p>"},{"location":"geograypher/prediction/#creating-training-data","title":"Creating training data","text":"<p>Geograypher can process the data from the first two steps and produce a folder of labels that can be used for training. These workflows are implemented in <code>render_mesh.py</code> and <code>render_labels.ipynb</code> for the image-based workflows and <code>orthomosaic_predictions.py</code> for orthomosiacs. For more information see the command line arguments of these files.</p>"},{"location":"geograypher/prediction/#training-models","title":"Training models","text":"<p>Once the training labels have been exported, the user is free to train any sort of prediction model on them using their own frameworks. The Geograypher project aims to produce training data and consume predictions, but is agnostic to what happens in between.</p> <p>OFO currently supports a workflow for making semantic segmentation predictions using the <code>mmsegmentation</code> framework. This open-source project built on <code>PyTorch</code> provides a standardized way to use a wide array of state-of-the-art semantic segmentation models interchangeably. To process the data into the required training format, you can use <code>segmentation_utils</code>. Specifically, <code>remap_classes.py</code> can be used to merge or relabel classes, and <code>folder_to_cityscapes.py</code> can be used to process a folder of labels into the required format. By default, this creates a config file that allows you to train a Segformer model on this data. This code can be run using the <code>mmseg-utils</code> conda environment.</p> <p>You can train a model using <code>mmseg/tools/train.py</code>. Simply point the script at the config file generated in the last step. This can be run using the <code>openmmlab</code> conda environment.</p>"},{"location":"geograypher/prediction/#generating-predictions","title":"Generating predictions","text":"<p>As discussed before, you can generated predictions from a ML model using whatever framework you want. If you want to use the model trained with <code>mmsegmentation</code> in the last step, you can use the <code>tools/inference.py</code> script in our fork of <code>mmsegmentation</code>. Point this at the trained model and the folder of images you want to generated predictions on. This will generated a folder of predictions structured the same way as the input data.</p> <p>You can visualize the predictions using <code>segmentation_utils visualize_semantic_labels.py</code>.</p>"},{"location":"geograypher/prediction/#aggregating-predictions","title":"Aggregating predictions","text":"<p>You can use <code>aggregate_viewpoints.py</code> or <code>aggregate_predictions.ipynb</code> to aggregate the predictions together into geo-spatial coordinates. If you want to do further analysis, you should export the predictions into a geofile.</p>"},{"location":"geograypher/prediction/#evaluating-predictions","title":"Evaluating predictions","text":""},{"location":"internal-docs/","title":"Internal OFO documentation","text":"<p>This section covers resources and tasks that support internal OFO development.</p>"},{"location":"internal-docs/data-version-control/","title":"Data version control","text":"<p>Notes and tips as we develop our data version control practices. This is still a work in progress!</p>"},{"location":"internal-docs/data-version-control/#datalad","title":"Datalad","text":""},{"location":"internal-docs/data-version-control/#clone-a-code-repository-containing-a-version-controlled-dataladgit-annex-dataset-as-a-submodule","title":"Clone a code repository containing a version-controlled (Datalad/git-annex) dataset as a submodule","text":"<p>To clone a code repo and also obtain datalad data submodules (\"subdatasets\" in Datalad terms) (just the data file pointers, not actual data):</p> <p><code>datalad install -r &lt;repo url&gt;</code></p> <p>This appears to clone the data submodules at the state (commit) referenced from the parent repository, even if there are more recent commits to the submodules. Optionally, add <code>-g</code> to get the actual data underlying the data file pointers from the remote repositories, if auto-enabled.</p> <p>(The above appears to be the same as <code>git clone --recursive &lt;repo url&gt;</code>.)</p> <p>If the code repository has already been cloned without the data submodules (e.g. via <code>git clone</code> without <code>--recursive</code>, or <code>datalad install</code> without <code>-r</code>), you can obtain the submodules from witihn the parent repo with:</p> <p><code>datalad install -r .</code> or from within the (empty) submodule folder with <code>datalad install .</code> or <code>datalad get -n</code>.</p> <p>(These both appear to clone the data submodules at the state (commit) referenced from the parent repository.)</p> <p>To check if your dataset (submodule) state is current with its remote: <code>git status</code></p> <p>(You can use <code>datalad status</code> but it does not check status relative to the remote, just checks for changes to local files.)</p> <p>To pull the most recent version (commit) of the dataset (submodule) (useful, for example, if you cloned a code repo that contained a data submodule frozen at a specific commit that has since been update):</p> <p><code>datalad update --how merge</code> or <code>git pull</code></p> <p>If you checked out a different commit of the dataset submodule and want to restore it to the version that is tagged in the parent code repo (assuming you haven't made a commit there to update the dataset version tag): <code>cd</code> to the parent code repo and run <code>git submodule update</code>, possibly with the <code>--init</code> option.</p> <p>(The following does not seem to work as documented: to pull the version (commit) of the dataset registered in the parent repo, run <code>datalad update --how merge --follow parentds</code> or potentially instead of <code>merge</code>, use <code>checkout</code>. )</p> <p>To enable access to the data files (in cloud storage) underlying a Datalad/git-annex dataset, see this section.</p>"},{"location":"internal-docs/data-version-control/#clone-existing-remote-dataset-with-metadata-on-github-and-files-on-cyverse-not-inside-a-code-repository","title":"Clone existing remote dataset (with metadata on GitHub and files on CyVerse), not inside a code repository","text":"<ul> <li><code>datalad clone &lt;github url&gt;</code></li> <li>Enable the 'siblings' (data storage remotes): run the <code>datalad siblings ...</code> command suggested in output of the <code>clone</code> command. For CyVerse WebDAV, enter username and password when prompted.<ul> <li>Note: Once I got an error <code>Failed to create the collection: Prompt dismissed..</code> and the solution (I'm not clear on why) was to first run <code>export PYTHON_KEYRING_BACKEND=keyring.backends.null.Keyring</code> per this post</li> </ul> </li> <li>Check that the siblings were added: <code>datalad siblings</code>. You should see <code>origin</code>, <code>here</code>, and <code>cyverse</code>. <code>(+)</code> means it contains data files, not just pointers/metadata.</li> <li>Set up your repo to push data to cyverse whenever you commit metadata changes to github (from the root of the repo): <code>datalad create-sibling-github -d . &lt;name-of-repo&gt; --publish-depends cyverse --existing reconfigure</code><ul> <li>It would be great if we could find a way to have this setting saved in the DataLad settings in the GitHub repo so all clones use it by default.</li> </ul> </li> <li>Locally download a file/folder/glob: <code>datalad get &lt;file/folder/glob&gt;</code></li> <li>Modify the data locally</li> <li>Push your changes datalad push --to origin<ul> <li>Note: I haven't figured out why it doesn't work to push data with <code>datalad push --to cyverse</code> the way it does if you're the one who created the dataset. (This is only relevant if you have not set up the repo to push to cyverse whenever committing to github as instructed above. However, a workaround appears to be <code>datalad push --data anything</code> though I'm not completely sure what this is intended for.</li> </ul> </li> <li>Pull remote changes from GitHub (pointers/metadata only): <code>datalad update --how merge</code></li> </ul>"},{"location":"internal-docs/data-version-control/#enable-access-to-the-data-files-in-cloud-storage-underlying-a-dataladgit-annex-dataset","title":"Enable access to the data files (in cloud storage) underlying a Datalad/git-annex dataset","text":"<p>To enable access to a \"data remote\" (\"data sibling\" in datalad terms; \"special remote\" in git-annex terms), <code>cd</code> to the repo and run <code>datalad siblings enable -s &lt;name of sibling&gt;</code>. To get a list of remotes that can be initialized, use <code>git annex initremote</code>. If you need to change anything about the data remote, or re-authenticate, see the docs for datalad sibling update or the docs for git-annex configremote.</p> <p>For the CyVerse WebDAV special remote, enter username and password when prompted.     * Note: Once I got an error <code>Failed to create the collection: Prompt dismissed..</code> and the solution (I'm not clear on why) was to first run <code>export PYTHON_KEYRING_BACKEND=keyring.backends.null.Keyring</code> per this post.</p> <p>If the data remote that you want to use is configured as an \"rclone remote\", you will need to install rclone, configure it to access the data remote as instructed by the dataset provider, and install the rclone special remote package for git-annex.</p>"},{"location":"internal-docs/data-version-control/#create-and-publish-a-dataladgit-annex-dataset","title":"Create and publish a Datalad/git-annex dataset","text":"<ul> <li>Create a dataset (git repo): <code>datalad create -c text2git &lt;name-of-dataset&gt;</code> and <code>cd</code> into it</li> <li>Optionally make larger text files be stored outside the git repo, in the 'annex' (by default the <code>text2git</code> config makes all text files be stored in the git repo): <code>nano .gitattributes</code> and change <code>...(mimeencoding=binary)and(largerthan=0)...</code> to <code>(mimeencoding=binary)or(largerthan=10kb)</code><ul> <li>Then commit this change to the git repo</li> </ul> </li> <li>Consider setting the hashing backend to be BLAKE2B160E for &gt; 2x faster hashing</li> <li>Copy data files into the repo (can use <code>datalad download-url</code> to track provenance in metadata)<ul> <li>Note to self: download raw imagery: <code>rclone copy gdrive:Natural_Reserves_Bucciarelli/KMZs original/KMZs --transfers 16 --progress --drive-shared-with-me</code></li> </ul> </li> <li>Check status: <code>datalad status</code></li> <li>Save (commit) its state: <code>datalad save -m \"&lt;message&gt;\"</code> (optionally specific specific files/folders to save)<ul> <li>It took about 30 min to save a dataset of ~6000 drone images. Interested to know if plain git-annex would be faster.</li> </ul> </li> <li>[Option 1] Create Cyverse remote ('sibling') for data: <code>WEBDAV_USERNAME=&lt;username&gt; WEBDAV_PASSWORD=&lt;password&gt; git annex initremote cyverse type=webdav url=https://data.cyverse.org/dav/iplant/projects/ofo/datalad-testing2 chunk=100mb encryption=none</code> (you can alternatively set the username and password env vars in advance of this call)</li> <li>[Option 2] Create a Box (or other <code>rclone</code>-enabled) remote for data:<ul> <li>Install rclone via <code>sudo apt install rclone</code></li> <li>Set up rclone for your Box account via <code>rclone config</code></li> <li>Run <code>sudo apt install git-annex-remote-rclone</code></li> <li>Run <code>git annex initremote box type=external externaltype=rclone chunk=100MiB encryption=none target=&lt;name of your remote in rclone&gt; prefix=&lt;path to the data folder in your Box account&gt;</code></li> </ul> </li> <li>Create github remote repo ('sibling') for metadata and small files: <code>datalad create-sibling-github -d . -s github-ofo ucnrs-data --github-organization open-forest-observatory --publish-depends cyverse &lt;optionally: another --publish-depends for other data remotes&gt;</code>. The <code>--publish-depends</code> part sets it up to trigger data upload to cyverse whenever the metadata is committed to github; otherwise you'd need two commands every time.<ul> <li>Note: Once I got an error <code>Failed to create the collection: Prompt dismissed..</code> and the solution (I'm not clear on why) was to first run <code>export PYTHON_KEYRING_BACKEND=keyring.backends.null.Keyring</code> per this post.</li> </ul> </li> <li>To set it up to autoenable the remotes upon <code>datalad install</code> performed by collaborators (so they don't have to run <code>datalad siblings enable</code>, when doing <code>git annex initremote</code>, add <code>autoenable=true</code>. This probably only makes sense for remotes not needing authentication, like a public-readable remote. If you've already added a remote without autoenable and you want to turn on autoenable, use <code>git annex configremote &lt;remote name&gt; autoenable=true</code>.</li> <li>Push the data and metadata to cyverse &amp; github: <code>datalad push --to github-ofo</code></li> </ul>"},{"location":"internal-docs/data-version-control/#add-a-datalad-dataset-as-a-subdataset-within-an-existing-git-repo","title":"Add a Datalad dataset as a subdataset within an existing git repo","text":"<ul> <li><code>cd</code> into your git repo</li> <li>Not sure if necessary: make your git repo a Datalad repo too: <code>datalad create --force --no-annex</code></li> <li>Clone the existing dataset into your top-level repo as a 'subdataset': <code>datalad clone --dataset . &lt;github url&gt;</code></li> <li><code>cd</code> into the new subdataset</li> <li>Enable siblings as described above</li> <li>Continue in above section at \"Set up your repo to push data to cyverse whenever you commit metadata changes to github\"</li> </ul>"},{"location":"internal-docs/data-version-control/#delete-a-remote","title":"Delete a remote","text":"<ul> <li>Edit the <code>.git/config</code> file and delete the respective remote entries.</li> </ul>"},{"location":"internal-docs/data-version-control/#notes-on-transfer-speeds","title":"Notes on transfer speeds","text":"<ul> <li>With 40 GB of data, mostly individual drone images<ul> <li>Box rclone remote with -J16: ~120 MB/s</li> <li>GIN remote with -J16: ~10 MB/s</li> <li>Cyverse WebDAV remote with -J5: ~2 MB/s</li> </ul> </li> <li>With 160 GB of data, composed of 4 zip archives (38G, 22G, 96G) and remotes set to use 100 MB chunks:<ul> <li>Box with -J16: ~180 MB/s (used one thread per original file, not chunk)</li> <li>GIN with -J16: ~4 MB/s (same as ^)</li> <li>Cyverse WebDav with -J5: ~45 MB/s (same as ^)</li> </ul> </li> <li>Same as above but using 500 MB chunks:<ul> <li>Box: 210 MB/s</li> <li>CyVerse: ~45 MB/s (each transfer -- of 3 total -- seemed to have only ~12 MB/s even excluding the per-file overhead, which is now a much smaller fraction of the total transfer time with the larger chunks)</li> </ul> </li> </ul> <p>Interrupted transfers properly resumed on all three remotes (at least from the last transferred chunk).</p>"},{"location":"internal-docs/data-version-control/#dvc","title":"DVC","text":""},{"location":"internal-docs/data-version-control/#notes-on-usage-challenges","title":"Notes on usage challenges","text":"<ul> <li><code>dvc pull</code> can be given paths inside tracked directories. You can specify the actual file you want (as opposed to the .dvc file). But how do you know the directory listing* in order to request them?<ul> <li>With git-annex you can see the file listing because all the files show up as broken symlinks (until you pull the data, when they become unbroken).</li> </ul> </li> <li>When you use <code>dvc pull</code> on a granular data sub-component of a tracked directory and then modify that file, are the granular file-level changes tracked anywhere?<ul> <li>No. DVC will tell you that the tracked directory was changed, but it seems you have to know which file was change in order to do <code>dvc add &lt;the-changed-file&gt;</code> (or maybe you can do <code>dvc add &lt;tracked-directory&gt;</code> and it will add the changes to the file. Also, it will not tell you what sub-files were changed, it only knows that something changed in the directory.</li> </ul> </li> <li>If you download part of the full data repo (even everything associated with one .dvc file) and then update it (or even don't update it), you can't just do a generic <code>dvc push</code> to push the changes and \"sync\" your local changes with the data storage remote. You get an \"unexpected error\" related to the data files that you never downloaded. Also, if the files you downloaded were a subset of the files within the directory tracked by, e.g., <code>folder1.dvc</code> and you then do <code>dvc push folder1</code>, this will have the unintended consequences of permanently deleting all the files in that folder other than the ones you did download. The only way to download, modify, and push a subset of files in a tracked folder is to explicitly push those files.</li> <li>When you add files/dirs to the repo, you have to explicitly <code>dvc add</code> each one (at whatever level you want them to be tracked), whereas git-annex can automatically track all of them; there are no separate units that are tracked, just all files.</li> <li>Added all images in a directory individually to DVC tracking with <code>dvc add *</code> but then could not find an equivalent <code>dvc remove *</code> to remove them all. Clearly adding all images individually is not the intended use method.</li> </ul>"},{"location":"internal-docs/data-version-control/#notes-to-self-for-config","title":"Notes to self for config","text":"<ul> <li><code>git init</code>, <code>dvc init</code>, add files to repo.</li> <li>dvc remote add -d cyverse webdavs://data.cyverse.org/dav/iplant/projects/ofo/datalad-testing4</li> <li>dvc remote modify --local cyverse_public user  <li>dvc remote modify --local cyverse_public password  <li>Mostly followed this</li>"},{"location":"internal-docs/jetstream/","title":"Using Jetstream2 for OFO development","text":""},{"location":"internal-docs/jetstream/#getting-set-up","title":"Getting set up","text":""},{"location":"internal-docs/jetstream/#new-users","title":"New users","text":"<p>These steps are required once per new user.</p> <ol> <li>Create an ACCESS account and provide your username to your supervisor, so they can add you to our project.</li> <li>[Possibly not required for web desktop use only:] Create (or locate an existing) SSH key. Here's a detailed tutorial with some good context (read sections 10.1 through 10.4). Note: this tutorial is designed for R and RStudio users, but you can ignore all the refenreces to R and RStudio and focus on the shell instructions.</li> <li>[Once added to our ACCESS project:] Add credentials in CACAO (after logging in with your ACCESS credentials):<ol> <li>The OFO ACCESS project (BIO220124):  go to Credentials (key icon), click Add Credential in the top right, and click Cloud Credential. Choose Jetstream2, log in at the ACCESS prompt, and select the project BIO220124.</li> <li>Your public SSH key: go to Credentials (key icon), click Add Credential in the top right, and click SSH Public Key. Enter a public key name (e.g. <code>derek-laptop01</code>) and paste in the key text [if you have skipped the step of creating a SSH key, just enter any dummy text here].</li> </ol> </li> <li>Add your public SSH key in Exosphere: Log in with your ACCESS credentials (once added to our project), go to our project/allocation (BIO220124), click Create in the top right, and click SSH Public Key. Enter a public key name (e.g. derek-laptop01) and paste in the key text [if you have skipped the step of creating a SSH key, just enter any dummy text here].</li> </ol>"},{"location":"internal-docs/jetstream/#new-vms","title":"New VMs","text":"<p>These steps are required once for each new VM that a user desires. This should be done during initial onboarding (after the new user steps above), and it can also be done whenever an updated VM is desired. There are several events that may trigger users to want a new VM:</p> <ul> <li>Jetstream2 updates their featured Ubuntu image</li> <li>OFO updates its development VM template (which is applied on top of the current Jetstream2 featured Ubuntu image)</li> <li>You need more compute nodes</li> <li>You screwed up your VM configuration and want to start fresh</li> </ul> <p>The steps to create a new instance(s) are:</p> <ol> <li>Go to CACAO and select the Deployments tab on the left. Click Add Deployment, select the OFO dev template from the list, and click Next.<ol> <li>For Instance Name, enter a name that's descriptive to you and starts with your name. For example, <code>derek-dev</code> or simply <code>derek</code>.</li> <li>For Boot Image Name, choose the most recent featured Ubuntu image.</li> <li>Enter the desired number of instances (usually 1).</li> <li>Enter a flavor (size). Start with m3.small or m3.quad, as you can always resize later when you need more compute.</li> <li>Enter the Manila share key and Metashape license server IP address found in the OFO Credentials google doc shared with all OFO members.</li> <li>Click Advanced, enable Configure Boot Disk, change Boot Type to Volume, and enter 60 GB unless you know you will need something larger. Normally you won't because most data is stored on <code>/ofo-share</code> which is not part of this 60 GB.</li> </ol> </li> <li>[Only necessary if you want to access RStudio Server; can be done at a later time:] SSH into the machine and change your password<ol> <li><code>ssh &lt;username&gt;@&lt;ip address&gt;</code></li> <li>Your username is the same as your ACCESS-CI username.</li> <li>If you lose track of the IP address, you can find it in CACAO: go to Deployments and select the deployment you want to access.</li> <li>Set a password for your user: <code>sudo passwd &lt;username&gt;</code>. Choose a secure password because if others can guess it they could access our VMs and shared data volume.</li> </ol> </li> </ol> <p>You should be the only one who can access this instance/VM, so it should be safe to store credentials on. Examples of credentials you might store are: a Box account login (stored in <code>rclone</code>; see below), so you can transfer files between the VM and Box; GitHub credentials, so you can push to repos without entering a password every time.</p>"},{"location":"internal-docs/jetstream/#basic-instance-access-and-management","title":"Basic instance access and management","text":""},{"location":"internal-docs/jetstream/#ssh-access","title":"SSH access","text":"<p>SSH into your instance using <code>ssh &lt;username&gt;@&lt;ip address&gt;</code>. You can find the IP address of your instance in CACAO under Deployments. Your username is the same as your ACCESS-CI username.</p>"},{"location":"internal-docs/jetstream/#remote-desktop-access","title":"Remote desktop access","text":"<p>To access a graphical remote desktop in a browser tab, navigate to that instance's page in CACAO under Deployments and click the  \"Web Desktop\" button.</p>"},{"location":"internal-docs/jetstream/#shelving-and-unshelving","title":"Shelving and unshelving","text":"<p>It is very important that you shelve your instances when not using them, especially the larger instances. </p> <p>It's fastest to this via the instance's page in Jetstream Exosphere:</p> <ol> <li>Go to our project/allocation (BIO220124)</li> <li>Click on Instances</li> <li>Clear the filter</li> <li>Click on your instance</li> <li>In the top right under \"Actions\", select \"Shelve\"</li> </ol> <p>Note that this has the same effect as powering off a computer; i.e. all unsaved work will be lost, and next time you will start from a fresh boot. Use this same approach to unshelve your instance when you're ready to resume work, and to resize your instance if you need more compute or memory (see below). Note that when unshelving via Exosphere, you may need to wait ~1 minute before you can open the remote desktop via CACAO.</p> <p>Shelving/unshelving can also be done via the CACAO interface under Deployments, by pressing the square Stop button to shelve it (or triangle Play button to unshelve it). Using this method though, unshelving takes much longer because it does a check that the VM has all the configurations specified by our template, a generally unnecessary step.</p>"},{"location":"internal-docs/jetstream/#resizing","title":"Resizing","text":"<p>Resizing is super useful to enable you do do simple tasks like coding and low-compute testing with a small instance (uses less of our allocation budget) and then sizing up when necessary for large compute jobs. Note that resizing involves a reboot, so any unsaved work will be lost. The <code>m2.small</code> instance size is sufficient for code development with lightweight processing.</p> <p>Resizing is not currently available in CACAO, but you can resize using an alternative interface called Exosphere. You will need to select our allocation (BIO220124), go to Instances, clear the filter, find your instance by name, and select it. Resize it via the Actions menu on the top right of the instance page.</p>"},{"location":"internal-docs/jetstream/#more-details","title":"More details","text":"<p>More details on instance access and management can be found in the Jetstream2 documentation, including:</p> <ul> <li>Instance Management Actions</li> </ul> <p>Note that some instance interactions available via the Exosphere interface are not available for instances created via CACAO.</p>"},{"location":"internal-docs/jetstream/#using-instances","title":"Using instances","text":""},{"location":"internal-docs/jetstream/#ofo-share","title":"<code>/ofo-share/</code>","text":"<p>Instances have very limited storage specific to the instance, but they are set up to mount an 8 TB shared volume at <code>/ofo-share/</code>. This is where all project data and code should be stored. For each project you work on, create (or use an existing) subfolder of <code>/ofo-share/</code> to hold the data.</p>"},{"location":"internal-docs/jetstream/#code-repositories-development","title":"Code repositories: development","text":"<p>For code development, each person should create a subfolder <code>/ofo-share/repos-&lt;firstname&gt;</code> to hold all their code repositories. Doing it this way (rather than everyone accessing the same code files) allows each person to work in their own branch and make their own changes that don't affect other people. The code you modify will be accessible to other people (via their own copies of the repo) when you push your commits.</p> <p>In contrast to code development, the data that the repo operates on should be in a general project data folder so that everyone can access it (so we don't need multiple copies of the data on <code>/ofo-share/</code>).</p>"},{"location":"internal-docs/jetstream/#code-repositories-production","title":"Code repositories: production","text":"<p>The folder <code>/ofo-share/utils</code> is intended to hold the current public, production code (generally the most recent commit in the <code>main</code> branch) of our code libraries. Use the code here if you need to use some of our OFO functions for an analysis (as opposed to actively developing/testing them, in which case they should be in your own <code>repos-&lt;firstname&gt;</code> folder). We can regularly <code>git pull</code> in these repos to keep them up to date with the remote, but don't directly edit the code in this folder or push from it.</p>"},{"location":"internal-docs/jetstream/#temporary-data","title":"Temporary data","text":"<p>For temporary data that you don't want to store in a project's data folder, you can use a folder <code>/ofo-share/scratch-&lt;firstname&gt;</code> or <code>/ofo-share/tmp/</code>.</p>"},{"location":"internal-docs/jetstream/#data-transfer","title":"Data transfer","text":"<p>For data files on cloud storage such as Box, an incredibly value command line tool is <code>rclone</code>. To set up an account, use <code>rclone config</code> and follow the instructions. Assuming you set up a Box account with the name <code>box_myname</code>, you'd transfer data with a command like: <code>rclone sync box_myname:projects/myproject_data/ /ofo-share/myproject_data --progress --transfers 16</code>.</p>"},{"location":"internal-docs/jetstream/#jupyter-notebook-jupyter-lab","title":"Jupyter Notebook / Jupyter Lab","text":"<p>You can get Jupyter Lab in a browser tab (accessible from anywhere on the internet) using Jupyter server on your instance. It's best if you start the server from the root of the <code>/ofo-share</code> folder so you have easy access to all files there. Run:</p> <pre><code>cd /ofo-share/\n/ofo-share/utils/environments/jetstream/jupyter-lab.sh\n</code></pre> <p>then copy the last URL shown in the terminal and paste it into a browser. Inside a notebook you should be able to choose different kernels (conda environments) that are configured for the user account running the server. The <code>base</code> conda environment is called <code>root</code> here. When you're done, shut down the notebook server (either from the notebook, or from the terminal by doing Control-C twice) to prevent access to the server by others. If you want a basic Jupyter notebook instead of Jupyter Lab, you can use <code>/ofo-share/utils/environments/jetstream/jupyter-notebook.sh</code>.</p>"},{"location":"internal-docs/jetstream/#rstudio","title":"RStudio","text":"<p>Use RStudio Server; don't use RStudio GUI via a remote desktop. RStudio Server has all the same functionality and many conveniences. RStudio Server is a version of RStudio that runs in a browser tab on your local machine, but all the data and compute comes from your remote VM. Access RStudio from the browser on your local computer at <code>https://&lt;ip address&gt;</code>. Log in using your ACCESS-CI username and the password you set up for it (in the \"Getting set up\" section above). The first time you access RStudio Server via HTTPS, you will likely get a warning about an unsigned SSL certificate, but that is OK; it's the certificate installed by CACAO upon VM creation and it enables a secure connection even though it's not signed. To interface with <code>git</code> for code development in RStudio, you can use the command line or use RStudio's built-in git functionality under the Git tab in the top-right pane (shows up when you've opened a project that is a git repo).</p> <p>Accessing <code>/ofo-share/</code> from the RStudio Server file browser: The RStudio Server file browser defaults to the <code>~/</code> directory. To access a higher-level directory on the instance (and specifically the <code>/ofo-share/</code> folder), you need to click the <code>...</code> in the top  right of the file browser and enter <code>/</code> at the prompt. Then you will see all top-level directories on the instance and you can navigate to <code>/ofo-share/</code>.</p>"},{"location":"internal-docs/jetstream/#vs-code-for-r","title":"VS Code for R","text":""},{"location":"internal-docs/jetstream/#connecting-to-remote-server-from-vs-code-first-time","title":"Connecting to remote server from VS Code: first time","text":"<ul> <li>Install the the VS Code extension \"Remote - SSH\"</li> <li>Connect to the remote VM by clicking the '&gt;&lt;' button in the bottom left</li> <li>Click 'Connect to host'.</li> <li>Click 'Add new SSH host'. Type <code>ssh &lt;your_access_username&gt;@&lt;vm_ip_address&gt;</code>.</li> <li>When prompted, choose to update the SSH config in <code>/home/&lt;username&gt;/.ssh/config</code></li> <li>Click the '&gt;&lt;' button again, then 'Connect to host...', then select the host you just added. You may need to repond 'Yes' to accept the fingerprint of the host.</li> </ul>"},{"location":"internal-docs/jetstream/#connecting-to-remote-server-from-vs-code-after-first-time","title":"Connecting to remote server from VS Code: after first time","text":"<ul> <li>Click the '&gt;&lt;' button, then 'Connect to host...', then select the host IP (or name) you want to connect to.</li> <li>Open a code folder on the remote host by clicking 'Open folder...'. You'll probably want to navigate up the directories (<code>..</code>) and into <code>/ofo-share/repos-&lt;yourname&gt;/</code>.</li> </ul>"},{"location":"internal-docs/jetstream/#setting-up-vs-code-for-r","title":"Setting up VS Code for R","text":"<p>Note that if you want to set up your local machine for R editing too, the steps below are not complete because some of them have already been completed on the OFO VM image. For full steps, see here.</p> <ul> <li>Open the plugins pane (click on the 4 blocks icon in the left)</li> <li>Find the <code>R</code> plugin. Install it locally. Then click 'Install in SSH'.</li> <li>Find the <code>R Debugger</code> plugin. Install it locally. Then click 'Install in SSH'.</li> <li>Set up the radian R terminal (it has autocompletion, syntax highlighting, etc): <ul> <li>In the extensions pane, click the gear next to the R extension, and click 'Extension settings'. Find the setting 'Rterm: Linux'. In the box, enter <code>/opt/radian</code>. NOTE: If your local machine is linux, this setting will also apply there. Therefore I recommend you put a symlink to your local radian binary (likely located at <code>~/.local/bin/radian</code>) at <code>/opt/radian</code>. This is how the VM is set up.</li> <li>Enable the extension's \"bracketed paste\" option: In the extension settings, find the setting 'Bracketed Paste' and check the box.</li> </ul> </li> <li>Code linting is enabled by default. To change what types of issues get flagged by the linter, see here.</li> </ul>"},{"location":"internal-docs/jetstream/#run-command-line-processes-without-staying-connected","title":"Run command line processes without staying connected","text":"<p>Many types of command line processes are long-running and you don't want to have to stay signed in via an SSH connection to keep them running. Examples include large <code>rclone</code> data transfers and long data processing runs. To create processes that stay running even when your SSH connection closes, one approach is to use the <code>tmux</code> or <code>screen</code> tools.</p> <p>To use <code>tmux</code>, in a SSH session, type <code>tmux</code> and hit enter at the prompt. Now you can run commands like usual, except that if you are disconnected (e.g. you close your terminal window), they continue running. You can reconnect by starting another SSH session and typing <code>tmux attach</code>. You can also run multiple processes (\"windows\") within one SSH session via <code>tmux</code>. To start a new one, type <code>Control-B C</code>. To move between windows, type <code>Control-B #</code>, where <code>#</code> is the index of the window (indicated in the bottom panel; starts at 0). To close one, switch to it and then type <code>Control-B &amp;</code>.</p> <p>To use <code>screen</code>, in a SSH session, type <code>screen</code> and hit enter at the prompt. Now you can run commands like usual, except that if you are disconnected (e.g. you close your terminal window), they continue running. You can reconnect by starting another SSH session and typing <code>screen -dr</code>. You can also run multiple processes (\"windows\") within one SSH session via <code>screen</code>. To start a new one, type <code>Control-A C</code>. To toggle between that one and the original, type <code>Control-A Control-A</code>. If you create more than two, select from them by typing <code>Control-A Shift-\"</code>. To close one, make it active (by selecting it from the menu that appears after typing <code>Control-A Shift-\"</code>) and then type <code>Control-A K</code>.</p>"},{"location":"internal-docs/jetstream/#use-gui-software-via-remote-desktop","title":"Use GUI software via remote desktop","text":"<p>Instances support a remote desktop that you access via a browser tab. To access it, navigate to the instance's page in CACAO and then click the Web Desktop icon. (If it does not load properly, try first opening a \"Web Shell\" from CACAO and closing it, and/or waiting 1 minute after unshelving.) Most software can be started via the \"Activities\" menu, which you can access with the Activities button in the top right (and then searching for the software name) or via the \"9 dots\" button in the bottom panel. Note that as configured, the bottom panel gets hidden when software windows overlap it, in which case you have to use the \"Activities\" button.</p> <p>Web browser: Firefox browser comes preinstalled as part of the JS2 featured image.</p>"},{"location":"internal-docs/jetstream/#metashape","title":"Metashape","text":"<p>The Metashape GUI is installed on the OFO dev image.</p> <p>The Metashape python module is also installed, specifically in the conda environment <code>meta</code>. For running Metashape and the <code>automate-metashape</code> repo code from the command line, switch to the environment using <code>conda activate meta</code>. We aim to keep the <code>meta</code> environment updated with the current version of Metashape.</p>"}]}